"""

Workflow currently uses .done markers for a once-through pipeline. Replace with
more specific file mappings per rule.

Workflow must be executed from the repo root folder (i.e. this folders parent)

"""

from pathlib import Path

WORKDIR = Path.cwd()


triggers = [
    "data/icen/.done",
    "data/oni/.done",
    "data/pop/.done",
    "data/shapefiles/.done",
    "data/spi6/.done",
    "data/climate/.done",
    "data/processed/province_01/.done",
    "data/processed/province_export_csv/.done",
    "data/processed/province_02b/.done",
    "data/models/baseline/.done",
    "data/models/bayes_hist/.done",
    "data/models/bayes/.done",
    "data/models/python/.done",
    "data/models/import_python/.done",
    "data/models/log_cases/.done",
    "data/models/dir/.done",
]

# Make sure container 'peru-build' is available

rule all:
    input:
        triggers

rule clean:
    run:
        for file_path in triggers:
            path = Path(file_path)
            try:
                path.unlink()
                print(f"Trigger '{file_path}' removed.")
            except FileNotFoundError:
                print(f"Cannot remove '{file_path}' (trigger not found).")
                pass

# =========================
# Dataset Rules
# =========================

rule dataset_icen:
    output: "data/icen/.done"
    shell:
        """
        docker build -t dataset-icen workflows/datasets/icen
        docker run --rm -v {WORKDIR}/data/icen:/app/data/icen dataset-icen
        touch {output}
        """

rule dataset_oni:
    output: "data/oni/.done"
    shell:
        """
        docker build -t dataset-icen workflows/datasets/oni
        docker run --rm -v {WORKDIR}/data/oni:/app/data/oni dataset-icen
        touch {output}
        """

rule dataset_proy:
    output: "data/pop/.done"
    shell:
        """
        docker build -t dataset-proy workflows/datasets/proy
        docker run --rm -v {WORKDIR}/data/pop:/app/data/pop dataset-proy
        touch {output}
        """

rule dataset_shapefiles:
    output: "data/shapefiles/.done"
    shell:
        """
        docker build -t dataset-shapefiles workflows/datasets/shapefiles
        docker run --rm -v {WORKDIR}/data/shapefiles:/app/data/shapefiles dataset-shapefiles
        touch {output}
        """

rule dataset_spi6:
    output: "data/spi6/.done"
    shell:
        """
        docker build -t dataset-spi6 workflows/datasets/spi6
        docker run --rm -v {WORKDIR}/data/spi6:/app/data/spi6 dataset-spi6
        touch {output}
        """

rule dataset_worldclim:
    output: "data/climate/.done"
    shell:
        """
        docker build -t dataset-worldclim workflows/datasets/worldclim
        docker run --rm -v {WORKDIR}/data/climate:/app/data/climate dataset-worldclim
        touch {output}
        """

# =========================
# Processing Rules
# =========================

rule proc_province:
    input: expand("data/{ds}/.done", ds=["icen", "oni", "pop", "shapefiles", "spi6", "climate"])
    output: "data/processed/province_01/.done"
    shell:
        """
        docker run --rm --platform linux/amd64 \
            -v {WORKDIR}/scripts:/app/scripts:ro \
            -v {WORKDIR}/workflows:/app/workflows:ro \
            -v {WORKDIR}/data:/app/data \
            peru-build \
            R -e 'source("scripts/processing/province_01.R"); source("scripts/processing/province_02.R")'
        touch {output}
        """

rule proc_province_export_csv:
    input: "data/processed/province_01/.done"
    output: "data/processed/province_export_csv/.done"
    shell:
        """
        docker run --rm --platform linux/amd64 \
            -v {WORKDIR}/scripts:/app/scripts:ro \
            -v {WORKDIR}/workflows:/app/workflows:ro \
            -v {WORKDIR}/data:/app/data \
            peru-build \
            R -e 'source("scripts/processing/province_export_csv.R")'
        touch {output}
        """

rule proc_province_02b:
    input: "data/processed/province_export_csv/.done"
    output: "data/processed/province_02b/.done"
    shell:
        """
        docker run --rm --platform linux/amd64 \
            -v {WORKDIR}/scripts:/app/scripts:ro \
            -v {WORKDIR}/workflows:/app/workflows:ro \
            -v {WORKDIR}/data:/app/data \
            peru-build \
            R -e 'source("scripts/processing/province_02b.R")'
        touch {output}
        """

# =========================
# Modeling Rules
# =========================

rule model_baseline:
    # input: "data/processed/province_02b/.done"
    output: "data/models/baseline/.done"
    shell:
        """
        docker run --rm --platform linux/amd64 \
            -v {WORKDIR}/scripts:/app/scripts:ro \
            -v {WORKDIR}/workflows:/app/workflows:ro \
            -v {WORKDIR}/data:/app/data \
            peru-build \
            R -e 'source("scripts/forecasting/province_baseline_forecaster.R")'
        touch {output}
        """

rule model_bayesian_historical:
    input: "data/models/baseline/.done"
    output: "data/models/bayes_hist/.done"
    shell:
        """
        docker run --rm --platform linux/amd64 \
            -v {WORKDIR}/scripts:/app/scripts:ro \
            -v {WORKDIR}/workflows:/app/workflows:ro \
            -v {WORKDIR}/data:/app/data \
            peru-build \
            R -e 'source("scripts/forecasting/province_historical_bayesian_forecasting.R")'
        touch {output}
        """

rule model_bayesian:
    input: "data/models/bayes_hist/.done"
    output: "data/models/bayes/.done"
    shell:
        """
        docker run --rm --platform linux/amd64 \
            -v {WORKDIR}/scripts:/app/scripts:ro \
            -v {WORKDIR}/workflows:/app/workflows:ro \
            -v {WORKDIR}/data:/app/data \
            peru-build \
            R -e 'source("scripts/forecasting/province_bayesian_forecasting.R")'
        touch {output}
        """

rule model_python_forecast:
    input: "data/models/bayes/.done"
    output: "data/models/python/.done"
    shell:
        """
        docker run --rm --platform linux/amd64 \
            -v {WORKDIR}/scripts:/app/scripts:ro \
            -v {WORKDIR}/workflows:/app/workflows:ro \
            -v {WORKDIR}/data:/app/data \
            peru-build \
            python scripts/forecasting/python_peru_forecast.py
        touch {output}
        """

rule model_import_python_forecast:
    input: "data/models/python/.done"
    output: "data/models/import_python/.done"
    shell:
        """
        docker run --rm --platform linux/amd64 \
            -v {WORKDIR}/scripts:/app/scripts:ro \
            -v {WORKDIR}/workflows:/app/workflows:ro \
            -v {WORKDIR}/data:/app/data \
            peru-build \
            R -e 'source("scripts/forecasting/province_python_forecasting.R")'
        touch {output}
        """

# =========================
# Ensemble Rules
# =========================

rule ensemble_log_cases:
    input: "data/models/import_python/.done"
    output: "data/models/log_cases/.done"
    shell:
        """
        docker run --rm --platform linux/amd64 \
            -v {WORKDIR}/scripts:/app/scripts:ro \
            -v {WORKDIR}/workflows:/app/workflows:ro \
            -v {WORKDIR}/data:/app/data \
            peru-build \
            R -e 'source("scripts/forecasting/province_log_cases.R")'
        touch {output}
        """

rule ensemble_dir_ensemble:
    input: "data/models/log_cases/.done"
    output: "data/models/dir/.done"
    shell:
        """
        docker run --rm --platform linux/amd64 \
            -v {WORKDIR}/scripts:/app/scripts:ro \
            -v {WORKDIR}/workflows:/app/workflows:ro \
            -v {WORKDIR}/data:/app/data \
            peru-build \
            R -e 'source("scripts/forecasting/province_dir_ensemble_scoring.R")'
        touch {output}
        """

# ======================================================================================
# Visualisation
# ======================================================================================

# This does not form part of the overall pipeline, but provides a rule to launch the
# dashboard.

# Note that running the dashboard in this way is NOT a good idea as it holds snakemake
# open until the streamlit app dies. But, it does provide a quick way to test it.

rule vis_streamlit:
    shell:
        """
        docker build -t streamlit {WORKDIR}/vis/streamlit
        docker run \
            -v {WORKDIR}/vis/streamlit:/app \
            -v {WORKDIR}/data:/app/data \
            -p 8501:8501 \
            streamlit
        """
