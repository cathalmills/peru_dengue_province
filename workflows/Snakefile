"""

Dengue prediction pipeline

Snakefile should be called from repo root

"""

import os
from pathlib import Path
from dotenv import load_dotenv


WORKDIR = Path.cwd()
SNAKEMAKE_CORES = os.environ.get("SNAKEMAKE_CORES", "1")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "")
TIMEGPT_KEY = os.environ.get("TIMEGPT_KEY", "")
RETRAIN_MODELS = os.environ.get("RETRAIN_MODELS", "false")  # speed up testing
DASHBOARD_PORT = os.environ.get("DASHBOARD_PORT", 8501)
MAX_WORKERS = os.environ.get("MAX_WORKERS", os.cpu_count())

load_dotenv()

triggers = [
    # Datasets
    "data/icen/.done",
    "data/oni/.done",
    "data/pop/.done",
    "data/shapefiles/.done",
    "data/spi6/.done",
    "data/worldclim/.done",
    # Processing
    "data/processed/province/.done",
    # Models
    "data/models/baseline/.done",
    "data/models/bayes/.done",
    "data/models/sarima/.done",
    "data/models/tcn/.done",
    "data/models/timegpt/.done",
    "data/models/python/.done",
    # Ensemble
    "data/models/log_cases/.done",
]

rule all:
    input:
        triggers

rule clean:
    run:
        for file_path in triggers:
            path = Path(file_path)
            try:
                path.unlink()
                print(f"Trigger '{file_path}' removed.")
            except FileNotFoundError:
                print(f"Cannot remove '{file_path}' (trigger not found).")
                pass

# =========================
# Dataset Rules
# =========================

rule dataset_icen:
    output: touch("data/icen/.done")
    shell:
        """
        mkdir -p {WORKDIR}/data/icen
        docker build -t dataset-icen workflows/datasets/icen
        docker run --rm -v {WORKDIR}/data/icen:/app/data/icen dataset-icen
        """

rule dataset_oni:
    output: touch("data/oni/.done")
    shell:
        """
        mkdir -p {WORKDIR}/data/oni
        docker build -t dataset-icen workflows/datasets/oni
        docker run --rm -v {WORKDIR}/data/oni:/app/data/oni dataset-icen
        """

rule dataset_proy:
    output: touch("data/pop/.done")
    shell:
        """
        mkdir -p {WORKDIR}/data/proy
        docker build -t dataset-proy workflows/datasets/proy
        docker run --rm -v {WORKDIR}/data/pop:/app/data/pop dataset-proy
        """

rule dataset_shapefiles:
    output: touch("data/shapefiles/.done")
    shell:
        """
        mkdir -p {WORKDIR}/data/shapefiles
        docker build -t dataset-shapefiles workflows/datasets/shapefiles
        docker run --rm -v {WORKDIR}/data/shapefiles:/app/data/shapefiles dataset-shapefiles
        """

rule dataset_spi6:
    output: touch("data/spi6/.done")
    shell:
        """
        mkdir -p {WORKDIR}/data/spi6
        docker build -t dataset-spi6 workflows/datasets/spi6
        docker run --rm -v {WORKDIR}/data/spi6:/app/data/spi6 dataset-spi6
        """

rule dataset_worldclim:
    output: touch("data/worldclim/.done")
    shell:
        """
        mkdir -p {WORKDIR}/data/climate
        docker build -t dataset-worldclim workflows/datasets/worldclim
        docker run --rm -v {WORKDIR}/data/climate:/app/data/climate dataset-worldclim
        """

# =========================
# Processing Rules
# =========================

rule proc_province:
    input: expand("data/{ds}/.done", ds=["icen", "oni", "pop", "shapefiles", "spi6", "worldclim"])
    output: touch("data/processed/province/.done")
    shell:
        """
        docker build --platform linux/amd64 -t peru-build .

        docker run --rm --platform linux/amd64 \
            -v {WORKDIR}/scripts:/app/scripts:ro \
            -v {WORKDIR}/workflows:/app/workflows:ro \
            -v {WORKDIR}/data:/app/data \
            peru-build \
            R -e 'source("scripts/processing/case_data.R")'

        docker run --rm --platform linux/amd64 \
            -v {WORKDIR}/scripts:/app/scripts:ro \
            -v {WORKDIR}/workflows:/app/workflows:ro \
            -v {WORKDIR}/data:/app/data \
            -e SNAKEMAKE_CORES="{SNAKEMAKE_CORES}" \
            peru-build \
            R -e 'source("scripts/processing/climate.R")'

        docker run --rm --platform linux/amd64 \
            -v {WORKDIR}/scripts:/app/scripts:ro \
            -v {WORKDIR}/workflows:/app/workflows:ro \
            -v {WORKDIR}/data:/app/data \
            peru-build \
            R -e 'source("scripts/processing/peru_neighbours.R")'
        """

rule extract_covars:
    input:
        "data/processed/province/.done",
    output:
        touch("data/covars/.done")
    shell:
        """
        docker build --platform linux/amd64 -t peru-build .
        docker run --rm --platform linux/amd64 \
            -v {WORKDIR}/scripts:/app/scripts:ro \
            -v {WORKDIR}/workflows:/app/workflows:ro \
            -v {WORKDIR}/data:/app/data \
            -v {WORKDIR}/predictions:/app/predictions \
            peru-build \
            R -e 'source("scripts/covars/extract_covars.R")'
        """

# =========================
# Modeling Rules
# =========================

rule model_baseline:
    input: "data/processed/province/.done"
    output: touch("data/models/baseline/.done")
    shell:
        """
        mkdir -p {WORKDIR}/predictions/baseline
        docker build --platform linux/amd64 -t peru-build .
        docker run --rm --platform linux/amd64 \
            -v {WORKDIR}/scripts:/app/scripts:ro \
            -v {WORKDIR}/workflows:/app/workflows:ro \
            -v {WORKDIR}/data:/app/data \
            -v {WORKDIR}/predictions/baseline:/app/predictions \
            peru-build \
            R -e 'source("scripts/forecasting/forecast_baseline.R")'
        """

rule model_bayes_climate:
    input: "data/processed/province/.done"
    output: touch("data/models/bayes/.done")
    shell:
        """
        mkdir -p {WORKDIR}/predictions/bayesian
        docker build --platform linux/amd64 -t peru-build .

        # # Historical predictions
        # docker run --rm --platform linux/amd64 \
        #     -v {WORKDIR}/scripts:/app/scripts:ro \
        #     -v {WORKDIR}/workflows:/app/workflows:ro \
        #     -v {WORKDIR}/data:/app/data \
        #     -v {WORKDIR}/predictions/bayesian:/app/predictions \
        #     -e SNAKEMAKE_CORES="{SNAKEMAKE_CORES}" \
        #     peru-build \
        #     R -e 'source("scripts/forecasting/historical_bayes.R")'

        # Forecasting
        docker run --rm --platform linux/amd64 \
            -v {WORKDIR}/scripts:/app/scripts:ro \
            -v {WORKDIR}/workflows:/app/workflows:ro \
            -v {WORKDIR}/data:/app/data \
            -v {WORKDIR}/predictions/bayesian:/app/predictions \
            -e SNAKEMAKE_CORES="{SNAKEMAKE_CORES}" \
            peru-build \
            R -e 'source("scripts/forecasting/forecast_bayes.R")'
        """

rule model_sarima:
    input: "data/processed/province/.done"
    output: touch("data/models/sarima/.done")
    params: max_workers=MAX_WORKERS
    shell:
        """
        docker build --platform linux/amd64 -t peru-build .

        # SARIMA models
        docker run --rm --platform linux/amd64 \
            -v {WORKDIR}/scripts:/app/scripts:ro \
            -v {WORKDIR}/workflows:/app/workflows:ro \
            -v {WORKDIR}/data:/app/data \
            -e MAX_WORKERS="{params.max_workers}" \
            peru-build \
            python scripts/forecasting/forecast_sarima.py
        """

rule model_tcn:
    input: "data/processed/province/.done"
    output: touch("data/models/tcn/.done")
    params:
        retrain_models=RETRAIN_MODELS,
    shell:
        """
        docker build --platform linux/amd64 -t peru-build .

        # TCN models
        docker run --rm --platform linux/amd64 \
            -v {WORKDIR}/scripts:/app/scripts:ro \
            -v {WORKDIR}/workflows:/app/workflows:ro \
            -v {WORKDIR}/data:/app/data \
            -e RETRAIN_MODELS="{params.retrain_models}" \
            peru-build \
            python scripts/forecasting/forecast_tcn.py
        """

rule model_timegpt:
    input: "data/processed/province/.done"
    output: touch("data/models/timegpt/.done")
    params:
        api_key=TIMEGPT_KEY,
        retrain_models=RETRAIN_MODELS,
    shell:
        """
        docker build --platform linux/amd64 -t peru-build .

        # TimeGPT models
        docker run --rm --platform linux/amd64 \
            -v {WORKDIR}/scripts:/app/scripts:ro \
            -v {WORKDIR}/workflows:/app/workflows:ro \
            -v {WORKDIR}/data:/app/data \
            -e TIMEGPT_KEY="{params.api_key}" \
            -e RETRAIN_MODELS="{params.retrain_models}" \
            peru-build \
            python scripts/forecasting/forecast_timegpt.py
        """

# =========================
# Ensemble Rules
# =========================

rule python_collate:
    input:
        "data/models/sarima/.done",
        "data/models/tcn/.done",
        "data/models/timegpt/.done",
    output: touch("data/models/python/.done")
    shell:
        """
        docker build --platform linux/amd64 -t peru-build .

        # Conversion back to R
        docker run --rm --platform linux/amd64 \
            -v {WORKDIR}/scripts:/app/scripts:ro \
            -v {WORKDIR}/workflows:/app/workflows:ro \
            -v {WORKDIR}/data:/app/data \
            -v {WORKDIR}/predictions:/app/predictions \
            peru-build \
            R -e 'source("scripts/forecasting/python_collate.R")'
        """

rule ensemble_log_cases:
    input:
        "data/models/baseline/.done",
        "data/models/bayes/.done",
        "data/models/python/.done",
    output:
        touch("data/models/log_cases/.done")
    shell:
        """
        docker build --platform linux/amd64 -t peru-build .
        docker run --rm --platform linux/amd64 \
            -v {WORKDIR}/scripts:/app/scripts:ro \
            -v {WORKDIR}/workflows:/app/workflows:ro \
            -v {WORKDIR}/data:/app/data \
            -v {WORKDIR}/predictions:/app/predictions \
            -v {WORKDIR}/analysis:/app/analysis \
            peru-build \
            R -e 'source("scripts/ensemble/log_cases.R")'
        """

# ======================================================================================
# Dashboard
# ======================================================================================

# This does not form part of the overall pipeline, but provides a rule to launch the
# dashboard.

# Note that running the dashboard in this way is NOT a good idea as it holds snakemake
# open until the streamlit app dies. But, it does provide a quick way to test it.
rule dashboard:
    params:
        port=DASHBOARD_PORT
    shell:
        """
        docker build -t streamlit {WORKDIR}/dashboard/streamlit
        docker run \
            -v {WORKDIR}/dashboard/streamlit:/app \
            -v {WORKDIR}/data:/app/data \
            -v {WORKDIR}/predictions:/app/predictions \
            -v {WORKDIR}/analysis:/app/analysis \
            -p {params.port}:8501 \
            streamlit
        """

# Start streamlit container in background
rule start_dashboard:
    params:
        port=DASHBOARD_PORT
    shell:
        """
        mkdir -p logs/dashboard
        docker build -t streamlit {WORKDIR}/dashboard/streamlit
        docker run \
            -v {WORKDIR}/dashboard/streamlit:/app \
            -v {WORKDIR}/data:/app/data \
            -v {WORKDIR}/predictions:/app/predictions \
            -v {WORKDIR}/analysis:/app/analysis \
            -p {params.port}:8501 \
            streamlit 2>&1 > logs/dashboard/streamlit.log > &
        """
