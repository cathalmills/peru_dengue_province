"""

Workflow currently uses .done markers for a once-through pipeline. Replace with
more specific file mappings per rule.

Workflow must be executed from the repo root folder (i.e. this folders parent)

"""

import os
from pathlib import Path
from dotenv import load_dotenv


WORKDIR = Path.cwd()
SNAKEMAKE_CORES = os.environ.get("SNAKEMAKE_CORES", "1")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "")

load_dotenv()

triggers = [
    # Datasets
    "data/icen/.done",
    "data/oni/.done",
    "data/pop/.done",
    "data/shapefiles/.done",
    "data/spi6/.done",
    "data/worldclim/.done",
    # Processing
    "data/processed/province/.done",
    "data/processed/province_export_csv/.done",
    # Models
    "data/models/baseline/.done",
    "data/models/bayes/.done",
    "data/models/python/.done",
    # Ensemble
    "data/models/log_cases/.done",
]

rule all:
    input:
        triggers

rule clean:
    run:
        for file_path in triggers:
            path = Path(file_path)
            try:
                path.unlink()
                print(f"Trigger '{file_path}' removed.")
            except FileNotFoundError:
                print(f"Cannot remove '{file_path}' (trigger not found).")
                pass

# =========================
# Dataset Rules
# =========================

rule dataset_icen:
    output: touch("data/icen/.done")
    shell:
        """
        mkdir -p {WORKDIR}/data/icen
        docker build -t dataset-icen workflows/datasets/icen
        docker run --rm -v {WORKDIR}/data/icen:/app/data/icen dataset-icen
        """

rule dataset_oni:
    output: touch("data/oni/.done")
    shell:
        """
        mkdir -p {WORKDIR}/data/oni
        docker build -t dataset-icen workflows/datasets/oni
        docker run --rm -v {WORKDIR}/data/oni:/app/data/oni dataset-icen
        """

rule dataset_proy:
    output: touch("data/pop/.done")
    shell:
        """
        mkdir -p {WORKDIR}/data/proy
        docker build -t dataset-proy workflows/datasets/proy
        docker run --rm -v {WORKDIR}/data/pop:/app/data/pop dataset-proy
        """

rule dataset_shapefiles:
    output: touch("data/shapefiles/.done")
    shell:
        """
        mkdir -p {WORKDIR}/data/shapefiles
        docker build -t dataset-shapefiles workflows/datasets/shapefiles
        docker run --rm -v {WORKDIR}/data/shapefiles:/app/data/shapefiles dataset-shapefiles
        """

rule dataset_spi6:
    output: touch("data/spi6/.done")
    shell:
        """
        mkdir -p {WORKDIR}/data/spi6
        docker build -t dataset-spi6 workflows/datasets/spi6
        docker run --rm -v {WORKDIR}/data/spi6:/app/data/spi6 dataset-spi6
        """

rule dataset_worldclim:
    output: touch("data/worldclim/.done")
    shell:
        """
        mkdir -p {WORKDIR}/data/worldclim
        docker build -t dataset-worldclim workflows/datasets/worldclim
        docker run --rm -v {WORKDIR}/data/climate:/app/data/climate dataset-worldclim
        """

# =========================
# Processing Rules
# =========================

rule proc_province:
    input: expand("data/{ds}/.done", ds=["icen", "oni", "pop", "shapefiles", "spi6", "worldclim"])
    output: touch("data/processed/province/.done")
    shell:
        """
        docker build --platform linux/amd64 -t peru-build .

        docker run --rm --platform linux/amd64 \
            -v {WORKDIR}/scripts:/app/scripts:ro \
            -v {WORKDIR}/workflows:/app/workflows:ro \
            -v {WORKDIR}/data:/app/data \
            -e SNAKEMAKE_CORES="{SNAKEMAKE_CORES}" \
            peru-build \
            R -e 'source("scripts/processing/province_01.R")'

        docker run --rm --platform linux/amd64 \
            -v {WORKDIR}/scripts:/app/scripts:ro \
            -v {WORKDIR}/workflows:/app/workflows:ro \
            -v {WORKDIR}/data:/app/data \
            peru-build \
            R -e 'source("scripts/processing/province_02.R")'
        """

rule proc_province_export_csv:
    input: "data/processed/province/.done"
    output: touch("data/processed/province_export_csv/.done")
    shell:
        """
        docker build --platform linux/amd64 -t peru-build .
        docker run --rm --platform linux/amd64 \
            -v {WORKDIR}/scripts:/app/scripts:ro \
            -v {WORKDIR}/workflows:/app/workflows:ro \
            -v {WORKDIR}/data:/app/data \
            peru-build \
            R -e 'source("scripts/processing/province_export_csv.R")'
        """

# =========================
# Modeling Rules
# =========================

rule model_baseline:
    input: "data/processed/province_export_csv/.done"
    output: touch("data/models/baseline/.done")
    shell:
        """
        mkdir -p {WORKDIR}/predictions_single/baseline
        docker build --platform linux/amd64 -t peru-build .
        docker run --rm --platform linux/amd64 \
            -v {WORKDIR}/scripts:/app/scripts:ro \
            -v {WORKDIR}/workflows:/app/workflows:ro \
            -v {WORKDIR}/data:/app/data \
            -v {WORKDIR}/predictions_single/baseline:/app/predictions \
            peru-build \
            R -e 'source("scripts/forecasting/province_baseline_forecaster.R")'
        """

rule model_bayes_climate:
    input: "data/processed/province_export_csv/.done"
    output: touch("data/models/bayes/.done")
    shell:
        """
        mkdir -p {WORKDIR}/predictions_single/bayesian
        docker build --platform linux/amd64 -t peru-build .

        # # Historical predictions
        # docker run --rm --platform linux/amd64 \
        #     -v {WORKDIR}/scripts:/app/scripts:ro \
        #     -v {WORKDIR}/workflows:/app/workflows:ro \
        #     -v {WORKDIR}/data:/app/data \
        #     -v {WORKDIR}/predictions_single/bayesian:/app/predictions \
        #     -e SNAKEMAKE_CORES="{SNAKEMAKE_CORES}" \
        #     peru-build \
        #     R -e 'source("scripts/forecasting/province_historical_bayesian_forecasting.R")'

        # Forecasting
        docker run --rm --platform linux/amd64 \
            -v {WORKDIR}/scripts:/app/scripts:ro \
            -v {WORKDIR}/workflows:/app/workflows:ro \
            -v {WORKDIR}/data:/app/data \
            -v {WORKDIR}/predictions_single/bayesian:/app/predictions \
            -e SNAKEMAKE_CORES="{SNAKEMAKE_CORES}" \
            peru-build \
            R -e 'source("scripts/forecasting/province_bayesian_forecasting.R")'
        """

# need to separate models into different folders
rule model_python:
    input: "data/processed/province_export_csv/.done"
    output: touch("data/models/python/.done")
    params:
        api_key=os.environ.get('TIMEGPT_KEY')
    shell:
        """
        docker build --platform linux/amd64 -t peru-build .

        # Python models
        docker run --rm --platform linux/amd64 \
            -v {WORKDIR}/scripts:/app/scripts:ro \
            -v {WORKDIR}/workflows:/app/workflows:ro \
            -v {WORKDIR}/data:/app/data \
            -e TIMEGPT_KEY={params.api_key} \
            peru-build \
            python scripts/forecasting/python_peru_forecast.py

        # Conversion back to R
        docker run --rm --platform linux/amd64 \
            -v {WORKDIR}/scripts:/app/scripts:ro \
            -v {WORKDIR}/workflows:/app/workflows:ro \
            -v {WORKDIR}/data:/app/data \
            -v {WORKDIR}/predictions_single:/app/predictions \
            peru-build \
            R -e 'source("scripts/forecasting/province_python_forecasting.R")'
        """

# =========================
# Ensemble Rules
# =========================

rule ensemble_log_cases:
    input:
        "data/models/baseline/.done",
        "data/models/bayes/.done",
        "data/models/python/.done",
    output:
        touch("data/models/log_cases/.done")
    shell:
        """
        docker build --platform linux/amd64 -t peru-build .
        docker run --rm --platform linux/amd64 \
            -v {WORKDIR}/scripts:/app/scripts:ro \
            -v {WORKDIR}/workflows:/app/workflows:ro \
            -v {WORKDIR}/data:/app/data \
            -v {WORKDIR}/predictions:/app/predictions \
            -v {WORKDIR}/analysis:/app/analysis \
            peru-build \
            R -e 'source("scripts/ensemble/province_log_cases.R")'
        """

# ======================================================================================
# Dashboard
# ======================================================================================

# This does not form part of the overall pipeline, but provides a rule to launch the
# dashboard.

# Note that running the dashboard in this way is NOT a good idea as it holds snakemake
# open until the streamlit app dies. But, it does provide a quick way to test it.
rule dashboard:
    shell:
        """
        docker build -t streamlit {WORKDIR}/dashboard/streamlit
        docker run \
            -v {WORKDIR}/dashboard/streamlit:/app \
            -v {WORKDIR}/data:/app/data \
            -v {WORKDIR}/predictions:/app/predictions \
            -v {WORKDIR}/analysis:/app/analysis \
            -e OPENAI_API_KEY={OPENAI_API_KEY} \
            -p 8501:8501 \
            streamlit
        """

# Start streamlit container in background
rule start_dashboard:
    shell:
        """
        mkdir -p logs/dashboard
        docker build -t streamlit {WORKDIR}/dashboard/streamlit
        docker run \
            -v {WORKDIR}/dashboard/streamlit:/app \
            -v {WORKDIR}/data:/app/data \
            -v {WORKDIR}/predictions:/app/predictions \
            -v {WORKDIR}/analysis:/app/analysis \
            -p 8501:8501 \
            streamlit 2>&1 > logs/dashboard/streamlit.log > &
        """
