"""

Workflow currently uses .done markers for a once-through pipeline. Replace with
more specific file mappings per rule.

Workflow must be executed from the repo root folder (i.e. this folders parent)

"""

from pathlib import Path

WORKDIR = Path.cwd()


triggers = [
    "data/icen/.done",
    "data/oni/.done",
    "data/pop/.done",
    "data/shapefiles/.done",
    "data/spi6/.done",
    "data/climate/.done",
    "data/processed/province/.done",
    "data/processed/province_export_csv/.done",
    "data/models/baseline/.done",
    "data/models/bayes/.done",
    "data/models/python/.done",
    "data/models/log_cases/.done",
]

# Make sure container 'peru-build' is available

rule all:
    input:
        triggers

rule clean:
    run:
        for file_path in triggers:
            path = Path(file_path)
            try:
                path.unlink()
                print(f"Trigger '{file_path}' removed.")
            except FileNotFoundError:
                print(f"Cannot remove '{file_path}' (trigger not found).")
                pass

# =========================
# Dataset Rules
# =========================

rule dataset_icen:
    output: touch("data/icen/.done")
    shell:
        """
        docker build -t dataset-icen workflows/datasets/icen
        docker run --rm -v {WORKDIR}/data/icen:/app/data/icen dataset-icen
        """

rule dataset_oni:
    output: touch("data/oni/.done")
    shell:
        """
        docker build -t dataset-icen workflows/datasets/oni
        docker run --rm -v {WORKDIR}/data/oni:/app/data/oni dataset-icen
        """

rule dataset_proy:
    output: touch("data/pop/.done")
    shell:
        """
        docker build -t dataset-proy workflows/datasets/proy
        docker run --rm -v {WORKDIR}/data/pop:/app/data/pop dataset-proy
        """

rule dataset_shapefiles:
    output: touch("data/shapefiles/.done")
    shell:
        """
        docker build -t dataset-shapefiles workflows/datasets/shapefiles
        docker run --rm -v {WORKDIR}/data/shapefiles:/app/data/shapefiles dataset-shapefiles
        """

rule dataset_spi6:
    output: touch("data/spi6/.done")
    shell:
        """
        docker build -t dataset-spi6 workflows/datasets/spi6
        docker run --rm -v {WORKDIR}/data/spi6:/app/data/spi6 dataset-spi6
        """

rule dataset_worldclim:
    output: touch("data/climate/.done")
    shell:
        """
        docker build -t dataset-worldclim workflows/datasets/worldclim
        docker run --rm -v {WORKDIR}/data/climate:/app/data/climate dataset-worldclim
        """

# =========================
# Processing Rules
# =========================

rule proc_province:
    input: expand("data/{ds}/.done", ds=["icen", "oni", "pop", "shapefiles", "spi6", "climate"])
    output: touch("data/processed/province/.done")
    shell:
        """
        docker run --rm --platform linux/amd64 \
            -v {WORKDIR}/scripts:/app/scripts:ro \
            -v {WORKDIR}/workflows:/app/workflows:ro \
            -v {WORKDIR}/data:/app/data \
            peru-build \
            R -e 'source("scripts/processing/province_01.R"); source("scripts/processing/province_02.R")'
        """

rule proc_province_export_csv:
    input: "data/processed/province/.done"
    output: touch("data/processed/province_export_csv/.done")
    shell:
        """
        docker run --rm --platform linux/amd64 \
            -v {WORKDIR}/scripts:/app/scripts:ro \
            -v {WORKDIR}/workflows:/app/workflows:ro \
            -v {WORKDIR}/data:/app/data \
            peru-build \
            R -e 'source("scripts/processing/province_export_csv.R")'
        """

# =========================
# Modeling Rules
# =========================

rule model_baseline:
    input: "data/processed/province_export_csv/.done"
    output: touch("data/models/baseline/.done")
    shell:
        """
        mkdir -p {WORKDIR}/predictions_single/baseline
        docker run --rm --platform linux/amd64 \
            -v {WORKDIR}/scripts:/app/scripts:ro \
            -v {WORKDIR}/workflows:/app/workflows:ro \
            -v {WORKDIR}/data:/app/data \
            -v {WORKDIR}/predictions_single/baseline:/app/predictions \
            peru-build \
            R -e 'source("scripts/forecasting/province_baseline_forecaster.R")'
        """

rule model_bayes_climate:
    input: "data/processed/province_export_csv/.done"
    output: touch("data/models/bayes/.done")
    shell:
        """
        mkdir -p {WORKDIR}/predictions_single/bayesian

        # Forecasting
        docker run --rm --platform linux/amd64 \
            -v {WORKDIR}/scripts:/app/scripts:ro \
            -v {WORKDIR}/workflows:/app/workflows:ro \
            -v {WORKDIR}/data:/app/data \
            -v {WORKDIR}/predictions_single/bayesian:/app/predictions \
            peru-build \
            R -e 'source("scripts/forecasting/province_bayesian_forecasting.R")'

        # Historical predictions
        docker run --rm --platform linux/amd64 \
            -v {WORKDIR}/scripts:/app/scripts:ro \
            -v {WORKDIR}/workflows:/app/workflows:ro \
            -v {WORKDIR}/data:/app/data \
            -v {WORKDIR}/predictions_single/bayesian:/app/predictions \
            peru-build \
            R -e 'source("scripts/forecasting/province_historical_bayesian_forecasting.R")'
        """

# need to separate models into different folders
rule model_python:
    input: "data/processed/province_export_csv/.done"
    output: touch("data/models/python/.done")
    shell:
        """
        # Python models
        docker run --rm --platform linux/amd64 \
            -v {WORKDIR}/scripts:/app/scripts:ro \
            -v {WORKDIR}/workflows:/app/workflows:ro \
            -v {WORKDIR}/data:/app/data \
            peru-build \
            python scripts/forecasting/python_peru_forecast.py

        # Conversion back to R
        docker run --rm --platform linux/amd64 \
            -v {WORKDIR}/scripts:/app/scripts:ro \
            -v {WORKDIR}/workflows:/app/workflows:ro \
            -v {WORKDIR}/data:/app/data \
            -v {WORKDIR}/predictions_single:/app/predictions \
            peru-build \
            R -e 'source("scripts/forecasting/province_python_forecasting.R")'
        """

# =========================
# Ensemble Rules
# =========================

rule ensemble_log_cases:
    # input:
    #     "data/models/baseline/.done",
    #     "data/models/bayes/.done",
    #     "data/models/python/.done",
    output:
        touch("data/models/log_cases/.done")
    shell:
        """
        docker run --rm --platform linux/amd64 \
            -v {WORKDIR}/scripts:/app/scripts:ro \
            -v {WORKDIR}/workflows:/app/workflows:ro \
            -v {WORKDIR}/data:/app/data \
            -v {WORKDIR}/predictions:/app/predictions \
            -v {WORKDIR}/analysis:/app/analysis \
            peru-build \
            R -e 'source("scripts/ensemble/province_log_cases.R")'
        """

# ======================================================================================
# Dashboard
# ======================================================================================

# This does not form part of the overall pipeline, but provides a rule to launch the
# dashboard.

# Note that running the dashboard in this way is NOT a good idea as it holds snakemake
# open until the streamlit app dies. But, it does provide a quick way to test it.
rule dashboard:
    shell:
        """
        docker build -t streamlit {WORKDIR}/dashboard/streamlit
        docker run \
            -v {WORKDIR}/dashboard/streamlit:/app \
            -v {WORKDIR}/data:/app/data \
            -v {WORKDIR}/predictions:/app/predictions \
            -v {WORKDIR}/analysis:/app/analysis \
            -p 8501:8501 \
            streamlit
        """

# Start streamlit container in background
rule start_dashboard:
    shell:
        """
        mkdir -p logs/dashboard
        docker build -t streamlit {WORKDIR}/dashboard/streamlit
        docker run \
            -v {WORKDIR}/dashboard/streamlit:/app \
            -v {WORKDIR}/data:/app/data \
            -v {WORKDIR}/predictions:/app/predictions \
            -v {WORKDIR}/analysis:/app/analysis \
            -p 8501:8501 \
            streamlit 2>&1 > logs/dashboard/streamlit.log > &
        """
